{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diyism/colab_kaldi2/blob/main/my_icefall_yes_no_dataset_recipe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AliAaueDNteG",
        "outputId": "315807ab-2f69-46b9-abc4-2806efaf1390",
        "collapsed": true
      },
      "source": [
        "#1. Install PyTorch, torchaudio, k2\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torchaudio\n",
        "import torchaudio\n",
        "print(torchaudio.__version__)\n",
        "\n",
        "#the cuda version and torch version should match the upper printed verions\n",
        "!pip install -q k2==1.24.4.dev20240905+cuda12.1.torch2.4.1 -f https://k2-fsa.github.io/k2/cuda.html\n",
        "!python3 -m k2.version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.1+cu121\n",
            "2.4.1+cu121\n",
            "Collecting environment information...\n",
            "\n",
            "k2 version: 1.24.4\n",
            "Build type: Release\n",
            "Git SHA1: cf664841c6d93e21e59b40aade84869b76c919c1\n",
            "Git date: Thu Sep 5 19:25:17 2024\n",
            "Cuda used to build k2: 12.1\n",
            "cuDNN used to build k2: \n",
            "Python version used to build k2: 3.10\n",
            "OS used to build k2: CentOS Linux release 7.9.2009 (Core)\n",
            "CMake version: 3.30.2\n",
            "GCC version: 9.3.1\n",
            "CMAKE_CUDA_FLAGS: -Wno-deprecated-gpu-targets -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_50,code=sm_50 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_60,code=sm_60 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_61,code=sm_61 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_70,code=sm_70 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_75,code=sm_75 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_86,code=sm_86 -DONNX_NAMESPACE=onnx_c2 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda -D_GLIBCXX_USE_CXX11_ABI=0 --compiler-options -Wall  --compiler-options -Wno-strict-overflow  --compiler-options -Wno-unknown-pragmas \n",
            "CMAKE_CXX_FLAGS:  -D_GLIBCXX_USE_CXX11_ABI=0 -Wno-unused-variable  -Wno-strict-overflow \n",
            "PyTorch version used to build k2: 2.4.1+cu121\n",
            "PyTorch is using Cuda: 12.1\n",
            "NVTX enabled: True\n",
            "With CUDA: True\n",
            "Disable debug: True\n",
            "Sync kernels : False\n",
            "Disable checks: False\n",
            "Max cpu memory allocate: 214748364800 bytes (or 200.0 GB)\n",
            "k2 abort: False\n",
            "__file__: /usr/local/lib/python3.10/dist-packages/k2/version/version.py\n",
            "_k2.__file__: /usr/local/lib/python3.10/dist-packages/_k2.cpython-310-x86_64-linux-gnu.so\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SsFZwCESWz_",
        "outputId": "78d302c2-d342-4898-c055-6acc89b45731"
      },
      "source": [
        "#2. install lhotse\n",
        "#Normally, we would use pip install lhotse. However, the yesno recipe is added recently and has not been released to PyPI yet,\n",
        "#so we install the latest unreleased version here.\n",
        "!pip install -q git+https://github.com/lhotse-speech/lhotse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP4RZ31xTKzL",
        "outputId": "ebca29cb-2a8d-4109-9de0-3bb031da72df"
      },
      "source": [
        "#3. install icefall\n",
        "#k2-icefall is a collection of Python scripts.You don't need to install it. What you need to do is to get its source code,\n",
        "#install its dependencies, and set the PYTHONPATH pointing to it.\n",
        "!git clone https://github.com/k2-fsa/icefall\n",
        "!pip install -q --upgrade onnxconverter-common\n",
        "!cd icefall && pip install -q -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'icefall'...\n",
            "remote: Enumerating objects: 18395, done.\u001b[K\n",
            "remote: Counting objects: 100% (703/703), done.\u001b[K\n",
            "remote: Compressing objects: 100% (384/384), done.\u001b[K\n",
            "remote: Total 18395 (delta 373), reused 548 (delta 278), pack-reused 17692 (from 1)\u001b[K\n",
            "Receiving objects: 100% (18395/18395), 20.29 MiB | 8.88 MiB/s, done.\n",
            "Resolving deltas: 100% (12469/12469), done.\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m595.8/595.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m678.1/678.1 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.1/66.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzyw8VyfUjUB",
        "outputId": "dcc91d3a-19ec-4d7c-8302-00d7021c5198"
      },
      "source": [
        "#4. data preparation\n",
        "\n",
        "# To remove the following warning message\n",
        "# 2023-07-27 05:03:07.156920: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
        "! pip uninstall -y tensorflow\n",
        "\n",
        "! export PYTHONPATH=/content/icefall:$PYTHONPATH && \\\n",
        "  cd /content/icefall/egs/yesno/ASR && \\\n",
        "  rm -rf data && \\\n",
        "  ./prepare.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-23 16:19:28 (prepare.sh:27:main) dl_dir: /content/icefall/egs/yesno/ASR/download\n",
            "2024-09-23 16:19:28 (prepare.sh:30:main) Stage 0: Download data\n",
            "/content/icefall/egs/yesno/ASR/download/waves_yesno.tar.gz: 100% 4.70M/4.70M [00:01<00:00, 3.52MB/s]\n",
            "2024-09-23 16:19:34 (prepare.sh:39:main) Stage 1: Prepare yesno manifest\n",
            "2024-09-23 16:19:37 (prepare.sh:45:main) Stage 2: Compute fbank for yesno\n",
            "2024-09-23 16:19:41,254 INFO [compute_fbank_yesno.py:65] Processing train\n",
            "Extracting and storing features: 100% 90/90 [00:00<00:00, 145.99it/s]\n",
            "2024-09-23 16:19:41,893 INFO [compute_fbank_yesno.py:65] Processing test\n",
            "Extracting and storing features: 100% 30/30 [00:00<00:00, 275.78it/s]\n",
            "2024-09-23 16:19:42 (prepare.sh:51:main) Stage 3: Prepare lang\n",
            "2024-09-23 16:19:48,195 INFO [prepare_lang_fst.py:174] Building standard CTC topology\n",
            "2024-09-23 16:19:48,196 INFO [prepare_lang_fst.py:183] Building L\n",
            "2024-09-23 16:19:48,197 INFO [prepare_lang_fst.py:191] Building HL\n",
            "2024-09-23 16:19:48,198 INFO [prepare_lang_fst.py:201] Skip building HLG\n",
            "2024-09-23 16:19:48 (prepare.sh:67:main) Stage 4: Prepare G\n",
            "/project/kaldilm/csrc/arpa_file_parser.cc:void kaldilm::ArpaFileParser::Read(std::istream&):79\n",
            "[I] Reading \\data\\ section.\n",
            "/project/kaldilm/csrc/arpa_file_parser.cc:void kaldilm::ArpaFileParser::Read(std::istream&):140\n",
            "[I] Reading \\1-grams: section.\n",
            "2024-09-23 16:19:48 (prepare.sh:93:main) Stage 5: Compile HLG\n",
            "2024-09-23 16:19:51,236 INFO [compile_hlg.py:124] Processing data/lang_phone\n",
            "2024-09-23 16:19:51,236 INFO [lexicon.py:171] Converting L.pt to Linv.pt\n",
            "/content/icefall/icefall/lexicon.py:172: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  L = k2.Fsa.from_dict(torch.load(lang_dir / \"L.pt\"))\n",
            "2024-09-23 16:19:51,247 INFO [compile_hlg.py:48] Building ctc_topo. max_token_id: 3\n",
            "/content/icefall/egs/yesno/ASR/./local/compile_hlg.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  L = k2.Fsa.from_dict(torch.load(f\"{lang_dir}/L_disambig.pt\"))\n",
            "2024-09-23 16:19:51,248 INFO [compile_hlg.py:52] Loading G.fst.txt\n",
            "2024-09-23 16:19:51,254 INFO [compile_hlg.py:62] Intersecting L and G\n",
            "2024-09-23 16:19:51,263 INFO [compile_hlg.py:64] LG shape: (4, None)\n",
            "2024-09-23 16:19:51,263 INFO [compile_hlg.py:66] Connecting LG\n",
            "2024-09-23 16:19:51,263 INFO [compile_hlg.py:68] LG shape after k2.connect: (4, None)\n",
            "2024-09-23 16:19:51,263 INFO [compile_hlg.py:70] <class 'torch.Tensor'>\n",
            "2024-09-23 16:19:51,263 INFO [compile_hlg.py:71] Determinizing LG\n",
            "2024-09-23 16:19:51,274 INFO [compile_hlg.py:74] <class '_k2.ragged.RaggedTensor'>\n",
            "2024-09-23 16:19:51,274 INFO [compile_hlg.py:76] Connecting LG after k2.determinize\n",
            "2024-09-23 16:19:51,274 INFO [compile_hlg.py:79] Removing disambiguation symbols on LG\n",
            "2024-09-23 16:19:51,281 INFO [compile_hlg.py:91] LG shape after k2.remove_epsilon: (6, None)\n",
            "2024-09-23 16:19:51,282 INFO [compile_hlg.py:96] Arc sorting LG\n",
            "2024-09-23 16:19:51,282 INFO [compile_hlg.py:99] Composing H and LG\n",
            "2024-09-23 16:19:51,283 INFO [compile_hlg.py:106] Connecting LG\n",
            "2024-09-23 16:19:51,283 INFO [compile_hlg.py:109] Arc sorting LG\n",
            "2024-09-23 16:19:51,284 INFO [compile_hlg.py:111] HLG.shape: (8, None)\n",
            "2024-09-23 16:19:51,284 INFO [compile_hlg.py:127] Saving HLG.pt to data/lang_phone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1phegInRZkbl",
        "outputId": "af055d49-b9d2-4f4f-a4b2-08e4aa9e2305",
        "collapsed": true
      },
      "source": [
        "#5.training\n",
        "\n",
        "! export PYTHONPATH=/content/icefall:$PYTHONPATH && \\\n",
        "  cd /content/icefall/egs/yesno/ASR && \\\n",
        "  ./tdnn/train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-23 16:29:58,170 INFO [train.py:481] Training started\n",
            "2024-09-23 16:29:58,170 INFO [train.py:482] {'exp_dir': PosixPath('tdnn/exp'), 'lang_dir': PosixPath('data/lang_phone'), 'lr': 0.01, 'feature_dim': 23, 'weight_decay': 1e-06, 'start_epoch': 0, 'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 10, 'reset_interval': 20, 'valid_interval': 10, 'beam_size': 10, 'reduction': 'sum', 'use_double_scores': True, 'world_size': 1, 'master_port': 12354, 'tensorboard': True, 'num_epochs': 15, 'seed': 42, 'feature_dir': PosixPath('data/fbank'), 'max_duration': 30.0, 'bucketing_sampler': False, 'num_buckets': 10, 'concatenate_cuts': False, 'duration_factor': 1.0, 'gap': 1.0, 'on_the_fly_feats': False, 'shuffle': False, 'return_cuts': True, 'num_workers': 2, 'env_info': {'k2-version': '1.24.4', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': 'cf664841c6d93e21e59b40aade84869b76c919c1', 'k2-git-date': 'Thu Sep 5 19:25:17 2024', 'lhotse-version': '1.28.0.dev+git.bc2c0a2.clean', 'torch-version': '2.4.1+cu121', 'torch-cuda-available': True, 'torch-cuda-version': '12.1', 'python-version': '3.10', 'icefall-git-branch': 'master', 'icefall-git-sha1': '5c04c312-clean', 'icefall-git-date': 'Fri Sep 20 04:38:52 2024', 'icefall-path': '/content/icefall', 'k2-path': '/usr/local/lib/python3.10/dist-packages/k2/__init__.py', 'lhotse-path': '/usr/local/lib/python3.10/dist-packages/lhotse/__init__.py', 'hostname': 'bdf1923fd1fe', 'IP address': '172.28.0.12'}}\n",
            "2024-09-23 16:29:58,171 INFO [lexicon.py:168] Loading pre-compiled data/lang_phone/Linv.pt\n",
            "/content/icefall/icefall/lexicon.py:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  L_inv = k2.Fsa.from_dict(torch.load(lang_dir / \"Linv.pt\"))\n",
            "2024-09-23 16:29:58,173 INFO [train.py:495] device: cuda:0\n",
            "2024-09-23 16:29:59,523 INFO [asr_datamodule.py:146] About to get train cuts\n",
            "2024-09-23 16:29:59,523 INFO [asr_datamodule.py:249] About to get train cuts\n",
            "2024-09-23 16:29:59,831 INFO [asr_datamodule.py:149] About to create train dataset\n",
            "2024-09-23 16:29:59,831 INFO [asr_datamodule.py:201] Using SimpleCutSampler.\n",
            "2024-09-23 16:29:59,831 INFO [asr_datamodule.py:207] About to create train dataloader\n",
            "2024-09-23 16:29:59,832 INFO [asr_datamodule.py:220] About to get test cuts\n",
            "2024-09-23 16:29:59,832 INFO [asr_datamodule.py:257] About to get test cuts\n",
            "2024-09-23 16:30:02,312 INFO [train.py:422] Epoch 0, batch 0, loss[loss=1.065, over 2436.00 frames. ], tot_loss[loss=1.065, over 2436.00 frames. ], batch size: 4\n",
            "2024-09-23 16:30:02,856 INFO [train.py:422] Epoch 0, batch 10, loss[loss=0.4585, over 2828.00 frames. ], tot_loss[loss=0.7087, over 22192.90 frames. ], batch size: 4\n",
            "2024-09-23 16:30:03,263 INFO [train.py:444] Epoch 0, validation loss=0.9258, over 18048.00 frames. \n",
            "2024-09-23 16:30:03,739 INFO [train.py:422] Epoch 0, batch 20, loss[loss=0.2619, over 2695.00 frames. ], tot_loss[loss=0.4859, over 34971.47 frames. ], batch size: 5\n",
            "2024-09-23 16:30:04,073 INFO [train.py:444] Epoch 0, validation loss=0.5176, over 18048.00 frames. \n",
            "2024-09-23 16:30:04,127 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-0.pt\n",
            "2024-09-23 16:30:04,188 INFO [train.py:422] Epoch 1, batch 0, loss[loss=0.2616, over 2436.00 frames. ], tot_loss[loss=0.2616, over 2436.00 frames. ], batch size: 4\n",
            "2024-09-23 16:30:04,642 INFO [train.py:422] Epoch 1, batch 10, loss[loss=0.125, over 2828.00 frames. ], tot_loss[loss=0.1652, over 22192.90 frames. ], batch size: 4\n",
            "2024-09-23 16:30:04,958 INFO [train.py:444] Epoch 1, validation loss=0.1384, over 18048.00 frames. \n",
            "2024-09-23 16:30:05,424 INFO [train.py:422] Epoch 1, batch 20, loss[loss=0.07746, over 2695.00 frames. ], tot_loss[loss=0.1224, over 34971.47 frames. ], batch size: 5\n",
            "2024-09-23 16:30:05,895 INFO [train.py:444] Epoch 1, validation loss=0.07555, over 18048.00 frames. \n",
            "2024-09-23 16:30:05,955 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-1.pt\n",
            "2024-09-23 16:30:06,037 INFO [train.py:422] Epoch 2, batch 0, loss[loss=0.08332, over 2436.00 frames. ], tot_loss[loss=0.08332, over 2436.00 frames. ], batch size: 4\n",
            "2024-09-23 16:30:06,714 INFO [train.py:422] Epoch 2, batch 10, loss[loss=0.04603, over 2828.00 frames. ], tot_loss[loss=0.05742, over 22192.90 frames. ], batch size: 4\n",
            "2024-09-23 16:30:07,217 INFO [train.py:444] Epoch 2, validation loss=0.0478, over 18048.00 frames. \n",
            "2024-09-23 16:30:07,911 INFO [train.py:422] Epoch 2, batch 20, loss[loss=0.03767, over 2695.00 frames. ], tot_loss[loss=0.04905, over 34971.47 frames. ], batch size: 5\n",
            "2024-09-23 16:30:08,251 INFO [train.py:444] Epoch 2, validation loss=0.03421, over 18048.00 frames. \n",
            "2024-09-23 16:30:08,290 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-2.pt\n",
            "2024-09-23 16:30:08,349 INFO [train.py:422] Epoch 3, batch 0, loss[loss=0.03589, over 2436.00 frames. ], tot_loss[loss=0.03589, over 2436.00 frames. ], batch size: 4\n",
            "2024-09-23 16:30:08,805 INFO [train.py:422] Epoch 3, batch 10, loss[loss=0.02467, over 2828.00 frames. ], tot_loss[loss=0.02893, over 22192.90 frames. ], batch size: 4\n",
            "2024-09-23 16:30:09,135 INFO [train.py:444] Epoch 3, validation loss=0.02727, over 18048.00 frames. \n",
            "2024-09-23 16:30:09,561 INFO [train.py:422] Epoch 3, batch 20, loss[loss=0.02306, over 2695.00 frames. ], tot_loss[loss=0.02754, over 34971.47 frames. ], batch size: 5\n",
            "2024-09-23 16:30:09,884 INFO [train.py:444] Epoch 3, validation loss=0.02426, over 18048.00 frames. \n",
            "2024-09-23 16:30:09,944 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-3.pt\n",
            "2024-09-23 16:30:10,033 INFO [train.py:422] Epoch 4, batch 0, loss[loss=0.02191, over 2436.00 frames. ], tot_loss[loss=0.02191, over 2436.00 frames. ], batch size: 4\n",
            "2024-09-23 16:30:10,503 INFO [train.py:422] Epoch 4, batch 10, loss[loss=0.01717, over 2828.00 frames. ], tot_loss[loss=0.0198, over 22192.90 frames. ], batch size: 4\n",
            "2024-09-23 16:30:10,840 INFO [train.py:444] Epoch 4, validation loss=0.02016, over 18048.00 frames. \n",
            "2024-09-23 16:30:11,281 INFO [train.py:422] Epoch 4, batch 20, loss[loss=0.0174, over 2695.00 frames. ], tot_loss[loss=0.01958, over 34971.47 frames. ], batch size: 5\n",
            "2024-09-23 16:30:11,602 INFO [train.py:444] Epoch 4, validation loss=0.01942, over 18048.00 frames. \n",
            "2024-09-23 16:30:11,641 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-4.pt\n",
            "2024-09-23 16:30:11,701 INFO [train.py:422] Epoch 5, batch 0, loss[loss=0.01753, over 2436.00 frames. ], tot_loss[loss=0.01753, over 2436.00 frames. ], batch size: 4\n",
            "2024-09-23 16:30:12,162 INFO [train.py:422] Epoch 5, batch 10, loss[loss=0.01362, over 2828.00 frames. ], tot_loss[loss=0.01546, over 22192.90 frames. ], batch size: 4\n",
            "2024-09-23 16:30:12,483 INFO [train.py:444] Epoch 5, validation loss=0.01607, over 18048.00 frames. \n",
            "2024-09-23 16:30:12,916 INFO [train.py:422] Epoch 5, batch 20, loss[loss=0.01476, over 2695.00 frames. ], tot_loss[loss=0.01664, over 34971.47 frames. ], batch size: 5\n",
            "2024-09-23 16:30:13,238 INFO [train.py:444] Epoch 5, validation loss=0.0142, over 18048.00 frames. \n",
            "2024-09-23 16:30:13,278 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-5.pt\n",
            "2024-09-23 16:30:13,339 INFO [train.py:422] Epoch 6, batch 0, loss[loss=0.01427, over 2436.00 frames. ], tot_loss[loss=0.01427, over 2436.00 frames. ], batch size: 4\n",
            "2024-09-23 16:30:13,778 INFO [train.py:422] Epoch 6, batch 10, loss[loss=0.01133, over 2828.00 frames. ], tot_loss[loss=0.01286, over 22192.90 frames. ], batch size: 4\n",
            "2024-09-23 16:30:14,111 INFO [train.py:444] Epoch 6, validation loss=0.01477, over 18048.00 frames. \n",
            "2024-09-23 16:30:14,550 INFO [train.py:422] Epoch 6, batch 20, loss[loss=0.01348, over 2695.00 frames. ], tot_loss[loss=0.01427, over 34971.47 frames. ], batch size: 5\n",
            "2024-09-23 16:30:14,861 INFO [train.py:444] Epoch 6, validation loss=0.01321, over 18048.00 frames. \n",
            "2024-09-23 16:30:14,920 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-6.pt\n",
            "2024-09-23 16:30:14,977 INFO [train.py:422] Epoch 7, batch 0, loss[loss=0.01291, over 2436.00 frames. ], tot_loss[loss=0.01291, over 2436.00 frames. ], batch size: 4\n",
            "2024-09-23 16:30:15,422 INFO [train.py:422] Epoch 7, batch 10, loss[loss=0.01035, over 2828.00 frames. ], tot_loss[loss=0.01192, over 22192.90 frames. ], batch size: 4\n",
            "2024-09-23 16:30:15,734 INFO [train.py:444] Epoch 7, validation loss=0.01283, over 18048.00 frames. \n",
            "2024-09-23 16:30:16,174 INFO [train.py:422] Epoch 7, batch 20, loss[loss=0.0128, over 2695.00 frames. ], tot_loss[loss=0.01315, over 34971.47 frames. ], batch size: 5\n",
            "2024-09-23 16:30:16,486 INFO [train.py:444] Epoch 7, validation loss=0.01221, over 18048.00 frames. \n",
            "2024-09-23 16:30:16,527 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-7.pt\n",
            "2024-09-23 16:30:16,584 INFO [train.py:422] Epoch 8, batch 0, loss[loss=0.01189, over 2436.00 frames. ], tot_loss[loss=0.01189, over 2436.00 frames. ], batch size: 4\n",
            "2024-09-23 16:30:17,041 INFO [train.py:422] Epoch 8, batch 10, loss[loss=0.009863, over 2828.00 frames. ], tot_loss[loss=0.01095, over 22192.90 frames. ], batch size: 4\n",
            "2024-09-23 16:30:17,354 INFO [train.py:444] Epoch 8, validation loss=0.01204, over 18048.00 frames. \n",
            "2024-09-23 16:30:17,778 INFO [train.py:422] Epoch 8, batch 20, loss[loss=0.01264, over 2695.00 frames. ], tot_loss[loss=0.01187, over 34971.47 frames. ], batch size: 5\n",
            "2024-09-23 16:30:18,179 INFO [train.py:444] Epoch 8, validation loss=0.01175, over 18048.00 frames. \n",
            "2024-09-23 16:30:18,235 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-8.pt\n",
            "2024-09-23 16:30:18,324 INFO [train.py:422] Epoch 9, batch 0, loss[loss=0.0114, over 2436.00 frames. ], tot_loss[loss=0.0114, over 2436.00 frames. ], batch size: 4\n",
            "2024-09-23 16:30:18,976 INFO [train.py:422] Epoch 9, batch 10, loss[loss=0.00957, over 2828.00 frames. ], tot_loss[loss=0.01077, over 22192.90 frames. ], batch size: 4\n",
            "2024-09-23 16:30:19,485 INFO [train.py:444] Epoch 9, validation loss=0.01132, over 18048.00 frames. \n",
            "2024-09-23 16:30:20,161 INFO [train.py:422] Epoch 9, batch 20, loss[loss=0.01243, over 2695.00 frames. ], tot_loss[loss=0.01155, over 34971.47 frames. ], batch size: 5\n",
            "2024-09-23 16:30:20,640 INFO [train.py:444] Epoch 9, validation loss=0.01141, over 18048.00 frames. \n",
            "2024-09-23 16:30:20,679 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-9.pt\n",
            "2024-09-23 16:30:20,738 INFO [train.py:422] Epoch 10, batch 0, loss[loss=0.01089, over 2436.00 frames. ], tot_loss[loss=0.01089, over 2436.00 frames. ], batch size: 4\n",
            "2024-09-23 16:30:21,196 INFO [train.py:422] Epoch 10, batch 10, loss[loss=0.009316, over 2828.00 frames. ], tot_loss[loss=0.01031, over 22192.90 frames. ], batch size: 4\n",
            "2024-09-23 16:30:21,515 INFO [train.py:444] Epoch 10, validation loss=0.01151, over 18048.00 frames. \n",
            "2024-09-23 16:30:21,943 INFO [train.py:422] Epoch 10, batch 20, loss[loss=0.012, over 2695.00 frames. ], tot_loss[loss=0.011, over 34971.47 frames. ], batch size: 5\n",
            "2024-09-23 16:30:22,272 INFO [train.py:444] Epoch 10, validation loss=0.0111, over 18048.00 frames. \n",
            "2024-09-23 16:30:22,314 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-10.pt\n",
            "2024-09-23 16:30:22,371 INFO [train.py:422] Epoch 11, batch 0, loss[loss=0.01074, over 2436.00 frames. ], tot_loss[loss=0.01074, over 2436.00 frames. ], batch size: 4\n",
            "2024-09-23 16:30:22,825 INFO [train.py:422] Epoch 11, batch 10, loss[loss=0.009215, over 2828.00 frames. ], tot_loss[loss=0.01016, over 22192.90 frames. ], batch size: 4\n",
            "2024-09-23 16:30:23,144 INFO [train.py:444] Epoch 11, validation loss=0.01115, over 18048.00 frames. \n",
            "2024-09-23 16:30:23,587 INFO [train.py:422] Epoch 11, batch 20, loss[loss=0.01199, over 2695.00 frames. ], tot_loss[loss=0.01086, over 34971.47 frames. ], batch size: 5\n",
            "2024-09-23 16:30:23,906 INFO [train.py:444] Epoch 11, validation loss=0.01091, over 18048.00 frames. \n",
            "2024-09-23 16:30:23,946 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-11.pt\n",
            "2024-09-23 16:30:24,007 INFO [train.py:422] Epoch 12, batch 0, loss[loss=0.01057, over 2436.00 frames. ], tot_loss[loss=0.01057, over 2436.00 frames. ], batch size: 4\n",
            "2024-09-23 16:30:24,473 INFO [train.py:422] Epoch 12, batch 10, loss[loss=0.009076, over 2828.00 frames. ], tot_loss[loss=0.01002, over 22192.90 frames. ], batch size: 4\n",
            "2024-09-23 16:30:24,790 INFO [train.py:444] Epoch 12, validation loss=0.01102, over 18048.00 frames. \n",
            "2024-09-23 16:30:25,230 INFO [train.py:422] Epoch 12, batch 20, loss[loss=0.01179, over 2695.00 frames. ], tot_loss[loss=0.01074, over 34971.47 frames. ], batch size: 5\n",
            "2024-09-23 16:30:25,543 INFO [train.py:444] Epoch 12, validation loss=0.01094, over 18048.00 frames. \n",
            "2024-09-23 16:30:25,582 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-12.pt\n",
            "2024-09-23 16:30:25,639 INFO [train.py:422] Epoch 13, batch 0, loss[loss=0.01054, over 2436.00 frames. ], tot_loss[loss=0.01054, over 2436.00 frames. ], batch size: 4\n",
            "2024-09-23 16:30:26,087 INFO [train.py:422] Epoch 13, batch 10, loss[loss=0.009022, over 2828.00 frames. ], tot_loss[loss=0.009985, over 22192.90 frames. ], batch size: 4\n",
            "2024-09-23 16:30:26,411 INFO [train.py:444] Epoch 13, validation loss=0.01084, over 18048.00 frames. \n",
            "2024-09-23 16:30:26,839 INFO [train.py:422] Epoch 13, batch 20, loss[loss=0.01172, over 2695.00 frames. ], tot_loss[loss=0.01074, over 34971.47 frames. ], batch size: 5\n",
            "2024-09-23 16:30:27,161 INFO [train.py:444] Epoch 13, validation loss=0.01083, over 18048.00 frames. \n",
            "2024-09-23 16:30:27,199 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-13.pt\n",
            "2024-09-23 16:30:27,258 INFO [train.py:422] Epoch 14, batch 0, loss[loss=0.01049, over 2436.00 frames. ], tot_loss[loss=0.01049, over 2436.00 frames. ], batch size: 4\n",
            "2024-09-23 16:30:27,706 INFO [train.py:422] Epoch 14, batch 10, loss[loss=0.008996, over 2828.00 frames. ], tot_loss[loss=0.009868, over 22192.90 frames. ], batch size: 4\n",
            "2024-09-23 16:30:28,026 INFO [train.py:444] Epoch 14, validation loss=0.01077, over 18048.00 frames. \n",
            "2024-09-23 16:30:28,460 INFO [train.py:422] Epoch 14, batch 20, loss[loss=0.01175, over 2695.00 frames. ], tot_loss[loss=0.01037, over 34971.47 frames. ], batch size: 5\n",
            "2024-09-23 16:30:28,775 INFO [train.py:444] Epoch 14, validation loss=0.01078, over 18048.00 frames. \n",
            "2024-09-23 16:30:28,814 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-14.pt\n",
            "2024-09-23 16:30:28,816 INFO [train.py:555] Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. decode a single sound file\n",
        "!pip install kaldifeat==1.25.5.dev20240914+cuda12.1.torch2.4.1  -f https://csukuangfj.github.io/kaldifeat/cuda.html\n",
        "\n",
        "! export PYTHONPATH=/content/icefall:$PYTHONPATH && \\\n",
        "  cd /content/icefall/egs/yesno/ASR && \\\n",
        "  ./tdnn/pretrained.py \\\n",
        "    --checkpoint /content/icefall/egs/yesno/ASR/tdnn/exp/epoch-14.pt \\\n",
        "    --words-file /content/icefall/egs/yesno/ASR/data/lang_phone/words.txt \\\n",
        "    --HLG /content/icefall/egs/yesno/ASR/data/lang_phone/HLG.pt \\\n",
        "    /content/icefall/egs/yesno/ASR/download/waves_yesno/0_0_1_0_1_0_0_1.wav"
      ],
      "metadata": {
        "id": "C8nrz1UiiQID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd8f7bf-a1cf-41c7-afd8-0756ebc89da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://csukuangfj.github.io/kaldifeat/cuda.html\n",
            "Requirement already satisfied: kaldifeat==1.25.5.dev20240914+cuda12.1.torch2.4.1 in /usr/local/lib/python3.10/dist-packages (1.25.5.dev20240914+cuda12.1.torch2.4.1)\n",
            "2024-09-23 16:50:45,387 INFO [pretrained.py:136] {'feature_dim': 23, 'num_classes': 4, 'sample_rate': 8000, 'search_beam': 20, 'output_beam': 8, 'min_active_states': 30, 'max_active_states': 10000, 'use_double_scores': True, 'checkpoint': '/content/icefall/egs/yesno/ASR/tdnn/exp/epoch-14.pt', 'words_file': '/content/icefall/egs/yesno/ASR/data/lang_phone/words.txt', 'HLG': '/content/icefall/egs/yesno/ASR/data/lang_phone/HLG.pt', 'sound_files': ['/content/icefall/egs/yesno/ASR/download/waves_yesno/0_0_1_0_1_0_0_1.wav']}\n",
            "2024-09-23 16:50:45,407 INFO [pretrained.py:142] device: cuda:0\n",
            "2024-09-23 16:50:45,407 INFO [pretrained.py:144] Creating model\n",
            "/content/icefall/egs/yesno/ASR/./tdnn/pretrained.py:151: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(args.checkpoint, map_location=\"cpu\")\n",
            "2024-09-23 16:50:45,530 INFO [pretrained.py:156] Loading HLG from /content/icefall/egs/yesno/ASR/data/lang_phone/HLG.pt\n",
            "/content/icefall/egs/yesno/ASR/./tdnn/pretrained.py:157: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  HLG = k2.Fsa.from_dict(torch.load(params.HLG, map_location=\"cpu\"))\n",
            "2024-09-23 16:50:45,533 INFO [pretrained.py:160] Constructing Fbank computer\n",
            "2024-09-23 16:50:45,533 INFO [pretrained.py:171] Reading sound files: ['/content/icefall/egs/yesno/ASR/download/waves_yesno/0_0_1_0_1_0_0_1.wav']\n",
            "2024-09-23 16:50:45,543 INFO [pretrained.py:177] Decoding started\n",
            "2024-09-23 16:50:46,622 INFO [pretrained.py:213] \n",
            "/content/icefall/egs/yesno/ASR/download/waves_yesno/0_0_1_0_1_0_0_1.wav:\n",
            "NO NO YES NO YES NO NO YES\n",
            "\n",
            "\n",
            "2024-09-23 16:50:46,622 INFO [pretrained.py:215] Decoding Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WS-i2RdYzrx",
        "outputId": "15e87dca-ab42-4457-ff1d-8a743e733a8e"
      },
      "source": [
        "#7. decode all waves\n",
        "! export PYTHONPATH=/content/icefall:$PYTHONPATH && \\\n",
        "  cd /content/icefall/egs/yesno/ASR && \\\n",
        "  ./tdnn/decode.py\n",
        "\n",
        "! cd /content/icefall/egs/yesno/ASR && \\\n",
        "  cat tdnn/exp/recogs-test_set.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-23 16:51:54,314 INFO [decode.py:262] Decoding started\n",
            "2024-09-23 16:51:54,314 INFO [decode.py:263] {'exp_dir': PosixPath('tdnn/exp'), 'lang_dir': PosixPath('data/lang_phone'), 'feature_dim': 23, 'search_beam': 20, 'output_beam': 8, 'min_active_states': 30, 'max_active_states': 10000, 'use_double_scores': True, 'epoch': 14, 'avg': 2, 'export': False, 'feature_dir': PosixPath('data/fbank'), 'max_duration': 30.0, 'bucketing_sampler': False, 'num_buckets': 10, 'concatenate_cuts': False, 'duration_factor': 1.0, 'gap': 1.0, 'on_the_fly_feats': False, 'shuffle': False, 'return_cuts': True, 'num_workers': 2, 'env_info': {'k2-version': '1.24.4', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': 'cf664841c6d93e21e59b40aade84869b76c919c1', 'k2-git-date': 'Thu Sep 5 19:25:17 2024', 'lhotse-version': '1.28.0.dev+git.bc2c0a2.clean', 'torch-version': '2.4.1+cu121', 'torch-cuda-available': True, 'torch-cuda-version': '12.1', 'python-version': '3.10', 'icefall-git-branch': 'master', 'icefall-git-sha1': '5c04c312-clean', 'icefall-git-date': 'Fri Sep 20 04:38:52 2024', 'icefall-path': '/content/icefall', 'k2-path': '/usr/local/lib/python3.10/dist-packages/k2/__init__.py', 'lhotse-path': '/usr/local/lib/python3.10/dist-packages/lhotse/__init__.py', 'hostname': 'bdf1923fd1fe', 'IP address': '172.28.0.12'}}\n",
            "2024-09-23 16:51:54,315 INFO [lexicon.py:168] Loading pre-compiled data/lang_phone/Linv.pt\n",
            "/content/icefall/icefall/lexicon.py:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  L_inv = k2.Fsa.from_dict(torch.load(lang_dir / \"Linv.pt\"))\n",
            "2024-09-23 16:51:54,316 INFO [decode.py:272] device: cuda:0\n",
            "/content/icefall/egs/yesno/ASR/./tdnn/decode.py:274: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  HLG = k2.Fsa.from_dict(torch.load(f\"{params.lang_dir}/HLG.pt\", map_location=\"cpu\"))\n",
            "2024-09-23 16:51:54,437 INFO [decode.py:290] averaging ['tdnn/exp/epoch-13.pt', 'tdnn/exp/epoch-14.pt']\n",
            "/content/icefall/icefall/checkpoint.py:166: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  avg = torch.load(filenames[0], map_location=device)[\"model\"]\n",
            "/content/icefall/icefall/checkpoint.py:181: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(filenames[i], map_location=device)[\"model\"]\n",
            "2024-09-23 16:51:54,441 INFO [asr_datamodule.py:220] About to get test cuts\n",
            "2024-09-23 16:51:54,441 INFO [asr_datamodule.py:257] About to get test cuts\n",
            "2024-09-23 16:51:56,189 INFO [decode.py:203] batch 0/?, cuts processed until now is 4\n",
            "2024-09-23 16:51:57,729 INFO [decode.py:240] The transcripts are stored in tdnn/exp/recogs-test_set.txt\n",
            "2024-09-23 16:51:57,729 INFO [utils.py:657] [test_set] %WER 0.42% [1 / 240, 0 ins, 1 del, 0 sub ]\n",
            "2024-09-23 16:51:57,731 INFO [decode.py:248] Wrote detailed error stats to tdnn/exp/errs-test_set.txt\n",
            "2024-09-23 16:51:57,731 INFO [decode.py:315] Done!\n",
            "0_0_0_1_0_0_0_1-0:\tref=['NO', 'NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'YES']\n",
            "0_0_0_1_0_0_0_1-0:\thyp=['NO', 'NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'YES']\n",
            "0_0_1_0_0_0_1_0-1:\tref=['NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'YES', 'NO']\n",
            "0_0_1_0_0_0_1_0-1:\thyp=['NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'YES', 'NO']\n",
            "0_0_1_0_0_1_1_1-2:\tref=['NO', 'NO', 'YES', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "0_0_1_0_0_1_1_1-2:\thyp=['NO', 'NO', 'YES', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "0_0_1_0_1_0_0_1-3:\tref=['NO', 'NO', 'YES', 'NO', 'YES', 'NO', 'NO', 'YES']\n",
            "0_0_1_0_1_0_0_1-3:\thyp=['NO', 'NO', 'YES', 'NO', 'YES', 'NO', 'NO', 'YES']\n",
            "0_0_1_1_0_0_0_1-4:\tref=['NO', 'NO', 'YES', 'YES', 'NO', 'NO', 'NO', 'YES']\n",
            "0_0_1_1_0_0_0_1-4:\thyp=['NO', 'NO', 'YES', 'YES', 'NO', 'NO', 'NO', 'YES']\n",
            "0_0_1_1_0_1_1_0-5:\tref=['NO', 'NO', 'YES', 'YES', 'NO', 'YES', 'YES', 'NO']\n",
            "0_0_1_1_0_1_1_0-5:\thyp=['NO', 'NO', 'YES', 'YES', 'NO', 'YES', 'YES', 'NO']\n",
            "0_0_1_1_1_0_0_0-6:\tref=['NO', 'NO', 'YES', 'YES', 'YES', 'NO', 'NO', 'NO']\n",
            "0_0_1_1_1_0_0_0-6:\thyp=['NO', 'NO', 'YES', 'YES', 'YES', 'NO', 'NO', 'NO']\n",
            "0_0_1_1_1_1_0_0-7:\tref=['NO', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'NO']\n",
            "0_0_1_1_1_1_0_0-7:\thyp=['NO', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'NO']\n",
            "0_1_0_0_0_1_0_0-8:\tref=['NO', 'YES', 'NO', 'NO', 'NO', 'YES', 'NO', 'NO']\n",
            "0_1_0_0_0_1_0_0-8:\thyp=['NO', 'YES', 'NO', 'NO', 'NO', 'YES', 'NO', 'NO']\n",
            "0_1_0_0_1_0_1_0-9:\tref=['NO', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'NO']\n",
            "0_1_0_0_1_0_1_0-9:\thyp=['NO', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'NO']\n",
            "0_1_0_1_0_0_0_0-10:\tref=['NO', 'YES', 'NO', 'YES', 'NO', 'NO', 'NO', 'NO']\n",
            "0_1_0_1_0_0_0_0-10:\thyp=['NO', 'YES', 'NO', 'YES', 'NO', 'NO', 'NO']\n",
            "0_1_0_1_1_1_0_0-11:\tref=['NO', 'YES', 'NO', 'YES', 'YES', 'YES', 'NO', 'NO']\n",
            "0_1_0_1_1_1_0_0-11:\thyp=['NO', 'YES', 'NO', 'YES', 'YES', 'YES', 'NO', 'NO']\n",
            "0_1_1_0_0_1_1_1-12:\tref=['NO', 'YES', 'YES', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "0_1_1_0_0_1_1_1-12:\thyp=['NO', 'YES', 'YES', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "0_1_1_1_0_0_1_0-13:\tref=['NO', 'YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO']\n",
            "0_1_1_1_0_0_1_0-13:\thyp=['NO', 'YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO']\n",
            "0_1_1_1_1_0_1_0-14:\tref=['NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'YES', 'NO']\n",
            "0_1_1_1_1_0_1_0-14:\thyp=['NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'YES', 'NO']\n",
            "1_0_0_0_0_0_0_0-15:\tref=['YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO']\n",
            "1_0_0_0_0_0_0_0-15:\thyp=['YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO']\n",
            "1_0_0_0_0_0_1_1-16:\tref=['YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'YES', 'YES']\n",
            "1_0_0_0_0_0_1_1-16:\thyp=['YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'YES', 'YES']\n",
            "1_0_0_1_0_1_1_1-17:\tref=['YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'YES', 'YES']\n",
            "1_0_0_1_0_1_1_1-17:\thyp=['YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'YES', 'YES']\n",
            "1_0_1_1_0_1_1_1-18:\tref=['YES', 'NO', 'YES', 'YES', 'NO', 'YES', 'YES', 'YES']\n",
            "1_0_1_1_0_1_1_1-18:\thyp=['YES', 'NO', 'YES', 'YES', 'NO', 'YES', 'YES', 'YES']\n",
            "1_0_1_1_1_1_0_1-19:\tref=['YES', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'YES']\n",
            "1_0_1_1_1_1_0_1-19:\thyp=['YES', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'YES']\n",
            "1_1_0_0_0_1_1_1-20:\tref=['YES', 'YES', 'NO', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "1_1_0_0_0_1_1_1-20:\thyp=['YES', 'YES', 'NO', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "1_1_0_0_1_0_1_1-21:\tref=['YES', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'YES']\n",
            "1_1_0_0_1_0_1_1-21:\thyp=['YES', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'YES']\n",
            "1_1_0_1_0_1_0_0-22:\tref=['YES', 'YES', 'NO', 'YES', 'NO', 'YES', 'NO', 'NO']\n",
            "1_1_0_1_0_1_0_0-22:\thyp=['YES', 'YES', 'NO', 'YES', 'NO', 'YES', 'NO', 'NO']\n",
            "1_1_0_1_1_0_0_1-23:\tref=['YES', 'YES', 'NO', 'YES', 'YES', 'NO', 'NO', 'YES']\n",
            "1_1_0_1_1_0_0_1-23:\thyp=['YES', 'YES', 'NO', 'YES', 'YES', 'NO', 'NO', 'YES']\n",
            "1_1_0_1_1_1_1_0-24:\tref=['YES', 'YES', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO']\n",
            "1_1_0_1_1_1_1_0-24:\thyp=['YES', 'YES', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO']\n",
            "1_1_1_0_0_1_0_1-25:\tref=['YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES']\n",
            "1_1_1_0_0_1_0_1-25:\thyp=['YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES']\n",
            "1_1_1_0_1_0_1_0-26:\tref=['YES', 'YES', 'YES', 'NO', 'YES', 'NO', 'YES', 'NO']\n",
            "1_1_1_0_1_0_1_0-26:\thyp=['YES', 'YES', 'YES', 'NO', 'YES', 'NO', 'YES', 'NO']\n",
            "1_1_1_1_0_0_1_0-27:\tref=['YES', 'YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO']\n",
            "1_1_1_1_0_0_1_0-27:\thyp=['YES', 'YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO']\n",
            "1_1_1_1_1_0_0_0-28:\tref=['YES', 'YES', 'YES', 'YES', 'YES', 'NO', 'NO', 'NO']\n",
            "1_1_1_1_1_0_0_0-28:\thyp=['YES', 'YES', 'YES', 'YES', 'YES', 'NO', 'NO', 'NO']\n",
            "1_1_1_1_1_1_1_1-29:\tref=['YES', 'YES', 'YES', 'YES', 'YES', 'YES', 'YES', 'YES']\n",
            "1_1_1_1_1_1_1_1-29:\thyp=['YES', 'YES', 'YES', 'YES', 'YES', 'YES', 'YES', 'YES']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lQFBS-KdIVx",
        "outputId": "3b46b712-8042-4eeb-dd02-a0c7bf6a11b3"
      },
      "source": [
        "#8. show the detailed WER\n",
        "! cd /content/icefall/egs/yesno/ASR && \\\n",
        "  cat tdnn/exp/errs-test_set.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "%WER = 0.42\n",
            "Errors: 0 insertions, 1 deletions, 0 substitutions, over 240 reference words (239 correct)\n",
            "Search below for sections starting with PER-UTT DETAILS:, SUBSTITUTIONS:, DELETIONS:, INSERTIONS:, PER-WORD STATS:\n",
            "\n",
            "PER-UTT DETAILS: corr or (ref->hyp)  \n",
            "0_0_0_1_0_0_0_1-0:\tNO NO NO YES NO NO NO YES\n",
            "0_0_1_0_0_0_1_0-1:\tNO NO YES NO NO NO YES NO\n",
            "0_0_1_0_0_1_1_1-2:\tNO NO YES NO NO YES YES YES\n",
            "0_0_1_0_1_0_0_1-3:\tNO NO YES NO YES NO NO YES\n",
            "0_0_1_1_0_0_0_1-4:\tNO NO YES YES NO NO NO YES\n",
            "0_0_1_1_0_1_1_0-5:\tNO NO YES YES NO YES YES NO\n",
            "0_0_1_1_1_0_0_0-6:\tNO NO YES YES YES NO NO NO\n",
            "0_0_1_1_1_1_0_0-7:\tNO NO YES YES YES YES NO NO\n",
            "0_1_0_0_0_1_0_0-8:\tNO YES NO NO NO YES NO NO\n",
            "0_1_0_0_1_0_1_0-9:\tNO YES NO NO YES NO YES NO\n",
            "0_1_0_1_0_0_0_0-10:\tNO YES NO YES NO NO NO (NO->*)\n",
            "0_1_0_1_1_1_0_0-11:\tNO YES NO YES YES YES NO NO\n",
            "0_1_1_0_0_1_1_1-12:\tNO YES YES NO NO YES YES YES\n",
            "0_1_1_1_0_0_1_0-13:\tNO YES YES YES NO NO YES NO\n",
            "0_1_1_1_1_0_1_0-14:\tNO YES YES YES YES NO YES NO\n",
            "1_0_0_0_0_0_0_0-15:\tYES NO NO NO NO NO NO NO\n",
            "1_0_0_0_0_0_1_1-16:\tYES NO NO NO NO NO YES YES\n",
            "1_0_0_1_0_1_1_1-17:\tYES NO NO YES NO YES YES YES\n",
            "1_0_1_1_0_1_1_1-18:\tYES NO YES YES NO YES YES YES\n",
            "1_0_1_1_1_1_0_1-19:\tYES NO YES YES YES YES NO YES\n",
            "1_1_0_0_0_1_1_1-20:\tYES YES NO NO NO YES YES YES\n",
            "1_1_0_0_1_0_1_1-21:\tYES YES NO NO YES NO YES YES\n",
            "1_1_0_1_0_1_0_0-22:\tYES YES NO YES NO YES NO NO\n",
            "1_1_0_1_1_0_0_1-23:\tYES YES NO YES YES NO NO YES\n",
            "1_1_0_1_1_1_1_0-24:\tYES YES NO YES YES YES YES NO\n",
            "1_1_1_0_0_1_0_1-25:\tYES YES YES NO NO YES NO YES\n",
            "1_1_1_0_1_0_1_0-26:\tYES YES YES NO YES NO YES NO\n",
            "1_1_1_1_0_0_1_0-27:\tYES YES YES YES NO NO YES NO\n",
            "1_1_1_1_1_0_0_0-28:\tYES YES YES YES YES NO NO NO\n",
            "1_1_1_1_1_1_1_1-29:\tYES YES YES YES YES YES YES YES\n",
            "\n",
            "SUBSTITUTIONS: count ref -> hyp\n",
            "\n",
            "DELETIONS: count ref\n",
            "1   NO\n",
            "\n",
            "INSERTIONS: count hyp\n",
            "\n",
            "PER-WORD STATS: word  corr tot_errs count_in_ref count_in_hyp\n",
            "NO   115 1 116 115\n",
            "YES   124 0 124 124\n"
          ]
        }
      ]
    }
  ]
}